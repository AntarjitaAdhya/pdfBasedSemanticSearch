# -*- coding: utf-8 -*-
"""pdfBasedSemanticSearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17CF8kLgF_wa06EVZ5tVUnHeicZiEo7Y-
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pypdf sentence-transformers qdrant-client transformers streamlit accelerate

PDF_FOLDER = "/content/drive/MyDrive/ncert_biology"

from pypdf import PdfReader

def extract_text_with_page(pdf_path):
    reader = PdfReader(pdf_path)
    pages = []
    for page_no, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            pages.append({
                "page": page_no + 1,
                "text": text
            })
    return pages

def chunk_text(text, chunk_size=500):
    words = text.split()
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

!pip uninstall -y transformers sentence-transformers accelerate
!pip install transformers==4.38.2 sentence-transformers==2.6.1 accelerate==0.27.2

from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance

client = QdrantClient(":memory:")

client.recreate_collection(
    collection_name="ncert_biology",
    vectors_config=VectorParams(
        size=384,
        distance=Distance.COSINE
    )
)

import os
from qdrant_client.models import PointStruct # Added import

points = []
idx = 0

for file in os.listdir(PDF_FOLDER):
    if file.endswith(".pdf"):
        pdf_path = os.path.join(PDF_FOLDER, file)
        pages = extract_text_with_page(pdf_path)

        for p in pages:
            chunks = chunk_text(p["text"])
            for chunk in chunks:
                vector = embedding_model.encode(chunk).tolist()

                points.append(
                    PointStruct(
                        id=idx, # Used PointStruct to define the point
                        vector=vector,
                        payload={
                            "pdf": file,
                            "page": p["page"],
                            "text": chunk
                        }
                    )
                )
                idx += 1

client.upsert(collection_name="ncert_biology", points=points)

print("âœ… PDFs indexed into Qdrant")

def retrieve_context(query, top_k=5):
    query_vector = embedding_model.encode(query).tolist()

    hits = client.search_points(
        collection_name="ncert_biology",
        vector=query_vector,
        limit=top_k
    )

    context = ""
    sources = []

    for hit in hits:
        context += hit.payload["text"] + "\n\n"
        sources.append(f"{hit.payload['pdf']} | Page {hit.payload['page']}")

    return context, sources

def retrieve_context(query, top_k=5):
    query_vector = embedding_model.encode(query).tolist()

    hits = client.search(
        collection_name="ncert_biology",
        query_vector=query_vector,
        limit=top_k
    )

    context = ""
    sources = []

    for hit in hits:
        context += hit.payload["text"] + "\n\n"
        sources.append(f"{hit.payload['pdf']} | Page {hit.payload['page']}")

    return context, sources

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

def generate_answer(question):
    if not question.strip():
        return "Please ask a question.", []

    context, sources = retrieve_context(question)

    if not context.strip():
        return "No relevant content found in PDFs.", []

    prompt = (
        "Answer the question using the context below.\n\n"
        f"Context:\n{context[:1500]}\n\n"
        f"Question:\n{question}\n\n"
        "Answer:"
    )

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )

    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=False
    )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return answer, sources

!pip install pyngrok

import gradio as gr

def ask(question):
    answer, sources = generate_answer(question)
    return answer, "\n".join(set(sources))

iface = gr.Interface(
    fn=ask,
    inputs="text",
    outputs=["text", "text"],
    title="ðŸ“˜ NCERT Biology â€“ AI PDF Search (RAG)"
)

iface.launch(share=True)